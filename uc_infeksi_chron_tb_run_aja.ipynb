{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import torch\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "x = torch.randn(1).cuda()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"configs/uc_infeksi_chron_tb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PARAMS = yaml.load(open(os.path.join(CONFIG_PATH, \"dataset.yaml\")), Loader=yaml.SafeLoader)\n",
    "PREPROCESSING_PARAMS = yaml.load(open(os.path.join(CONFIG_PATH, \"preprocessing.yaml\")), Loader=yaml.SafeLoader)\n",
    "MODEL_NAME = yaml.load(open(os.path.join(CONFIG_PATH, \"model.yaml\")), Loader=yaml.SafeLoader)['MODEL_NAME']\n",
    "MODEL_PARAMS = yaml.load(open(os.path.join(CONFIG_PATH, \"model.yaml\")), Loader=yaml.SafeLoader)[MODEL_NAME]\n",
    "TRAINING_PARAMS = yaml.load(open(os.path.join(CONFIG_PATH, \"training.yaml\")), Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories =  DATASET_PARAMS['CATEGORIES']\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATASET_DIR = os.path.join(BASE_DIR, DATASET_PARAMS['DATA_PATH'])\n",
    "\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_DIR, \"val\")\n",
    "TEST_DIR = os.path.join(DATASET_DIR, \"test\")\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_device, set_seed\n",
    "\n",
    "set_seed(DATASET_PARAMS['SEED'])\n",
    "DEVICE = get_device()\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset structure must be like this:\n",
    "\n",
    "XDL_Collitis/\n",
    "├── datasets/\n",
    "│   ├── uc/\n",
    "│   │   ├── limuc/\n",
    "│   │   │   ├── uc_1/\n",
    "│   │   │   ├── uc_2/\n",
    "│   │   │   └── uc_3/\n",
    "│   │   ├── changsu/\n",
    "│   │   │   ├── uc_1/\n",
    "│   │   │   ├── uc_2/\n",
    "│   │   │   └── uc_3/\n",
    "│   │   └── hyperkvasir/\n",
    "│   │       ├── uc_1/\n",
    "│   │       ├── uc_2/\n",
    "│   │       └── uc_3/\n",
    "│   └── infeksi/\n",
    "│       └── infeksi_non_spesifik/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_filenames_df, val_filenames_df, test_filenames_df = split_dataset(DATASET_DIR, \n",
    "#                                                                         categories, \n",
    "#                                                                         DATASET_PARAMS['UC_SOURCE'], \n",
    "#                                                                         shuffle=DATASET_PARAMS['IS_SHUFFLE'], \n",
    "#                                                                         seed=DATASET_PARAMS['SEED'], \n",
    "#                                                                         split_ratio=DATASET_PARAMS['SPLIT_RATIO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_filenames = {\"image_path\": [], \"class\": [], \"source\": []}\n",
    "\n",
    "uc_directory = os.path.join(DATASET_DIR, \"uc\")\n",
    "\n",
    "uc_subdirectories = os.listdir(uc_directory)\n",
    "\n",
    "for subdirectory in uc_subdirectories:\n",
    "    print(f\"subdirectory: {subdirectory}\")\n",
    "    subdirectory_path = os.path.join(uc_directory, subdirectory)\n",
    "    \n",
    "    # Walk through each subdirectory to find images\n",
    "    for root, dirs, files in os.walk(subdirectory_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                uc_filenames[\"image_path\"].append(image_path)\n",
    "                uc_filenames[\"class\"].append(\"uc\")\n",
    "                uc_filenames[\"source\"].append(subdirectory)  # Use the subdirectory name as the source\n",
    "\n",
    "uc_filenames_df = pd.DataFrame(uc_filenames)\n",
    "uc_filenames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infeksi_filenames = {\"image_path\": [], \"class\": [], \"source\": []}\n",
    "\n",
    "infeksi_directory = os.path.join(DATASET_DIR, \"infeksi\")\n",
    "\n",
    "infeksi_subdirectories = os.listdir(infeksi_directory)\n",
    "\n",
    "for subdirectory in infeksi_subdirectories:\n",
    "    print(f\"subdirectory: {subdirectory}\")\n",
    "    subdirectory_path = os.path.join(infeksi_directory, subdirectory)\n",
    "    \n",
    "    # Walk through each subdirectory to find images\n",
    "    for root, dirs, files in os.walk(subdirectory_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                infeksi_filenames[\"image_path\"].append(image_path)\n",
    "                infeksi_filenames[\"class\"].append(\"infeksi\")\n",
    "                infeksi_filenames[\"source\"].append(subdirectory)  # Use the subdirectory name as the source\n",
    "\n",
    "infeksi_filenames_df = pd.DataFrame(infeksi_filenames)\n",
    "infeksi_filenames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infeksi_filenames = {\"image_path\": [], \"class\": [], \"source\": []}\n",
    "\n",
    "# infeksi_directory = os.path.join(DATASET_DIR, \"infeksi\")\n",
    "\n",
    "# infeksi_subdirectories = os.listdir(infeksi_directory)\n",
    "\n",
    "# for subdirectory in infeksi_subdirectories:\n",
    "#     print(f\"subdirectory: {subdirectory}\")\n",
    "#     files = os.listdir(os.path.join(infeksi_directory, subdirectory))\n",
    "\n",
    "#     for file in files:\n",
    "#         infeksi_filenames[\"image_path\"].append(os.path.join(infeksi_directory, subdirectory, file))\n",
    "#         infeksi_filenames[\"class\"].append('infeksi')\n",
    "#         infeksi_filenames[\"source\"].append(subdirectory)\n",
    "\n",
    "# infeksi_filenames_df = pd.DataFrame(infeksi_filenames)\n",
    "# infeksi_filenames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_filename_from_path(path):\n",
    "#     return path.split('\\\\')[-1]\n",
    "\n",
    "# infeksi_filenames_df['filename'] = infeksi_filenames_df['image_path'].apply(get_filename_from_path)\n",
    "# infeksi_indonesia_test_filenames_df = infeksi_filenames_df[infeksi_filenames_df['source'] == 'infeksi test set']\n",
    "\n",
    "# infeksi_filtered_filenames_df = infeksi_filenames_df[~infeksi_filenames_df['filename'].isin(infeksi_indonesia_test_filenames_df['filename'])]\n",
    "# infeksi_filtered_filenames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chron_filenames = {\"image_path\": [], \"class\": [], \"source\": []}\n",
    "\n",
    "chron_directory = os.path.join(DATASET_DIR, \"chron\")\n",
    "\n",
    "chron_subdirectories = os.listdir(chron_directory)\n",
    "\n",
    "for subdirectory in chron_subdirectories:\n",
    "    print(f\"subdirectory: {subdirectory}\")\n",
    "    subdirectory_path = os.path.join(chron_directory, subdirectory)\n",
    "    \n",
    "    # Walk through each subdirectory to find images\n",
    "    for root, dirs, files in os.walk(subdirectory_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                chron_filenames[\"image_path\"].append(image_path)\n",
    "                chron_filenames[\"class\"].append(\"chron\")\n",
    "                chron_filenames[\"source\"].append(subdirectory)  # Use the subdirectory name as the source\n",
    "\n",
    "chron_filenames_df = pd.DataFrame(chron_filenames)\n",
    "chron_filenames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_filenames = {\"image_path\": [], \"class\": [], \"source\": []}\n",
    "\n",
    "tb_directory = os.path.join(DATASET_DIR, \"tb\")\n",
    "\n",
    "tb_subdirectories = os.listdir(tb_directory)\n",
    "\n",
    "for subdirectory in tb_subdirectories:\n",
    "    print(f\"subdirectory: {subdirectory}\")\n",
    "    subdirectory_path = os.path.join(tb_directory, subdirectory)\n",
    "    \n",
    "    # Walk through each subdirectory to find images\n",
    "    for root, dirs, files in os.walk(subdirectory_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                tb_filenames[\"image_path\"].append(image_path)\n",
    "                tb_filenames[\"class\"].append(\"tb\")\n",
    "                tb_filenames[\"source\"].append(subdirectory)  # Use the subdirectory name as the source\n",
    "\n",
    "tb_filenames_df = pd.DataFrame(tb_filenames)\n",
    "tb_filenames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# UC\n",
    "uc_trainval_filenames_df, uc_test_filenames_df = train_test_split(uc_filenames_df,\n",
    "                                                                  test_size=DATASET_PARAMS['SPLIT_RATIO'][-1],\n",
    "                                                                  random_state=DATASET_PARAMS['SEED'],\n",
    "                                                                  stratify=uc_filenames_df['source'])\n",
    "\n",
    "uc_train_filenames_df, uc_val_filenames_df = train_test_split(uc_trainval_filenames_df,\n",
    "                                                              test_size=DATASET_PARAMS['SPLIT_RATIO'][-2] * 1 / (1 - DATASET_PARAMS['SPLIT_RATIO'][-1]),\n",
    "                                                              random_state=DATASET_PARAMS['SEED'],\n",
    "                                                              stratify=uc_trainval_filenames_df['source'])\n",
    "\n",
    "# Infeksi\n",
    "# infeksi_train_filenames_df, infeksi_val_filenames_df = train_test_split(infeksi_filtered_filenames_df,\n",
    "#                                                                         test_size=DATASET_PARAMS['SPLIT_RATIO'][-2],\n",
    "#                                                                         random_state=DATASET_PARAMS['SEED'],\n",
    "#                                                                         stratify=infeksi_filtered_filenames_df['source'])\n",
    "\n",
    "# infeksi_test_filenames_df = infeksi_indonesia_test_filenames_df\n",
    "\n",
    "infeksi_trainval_filenames_df, infeksi_test_filenames_df = train_test_split(infeksi_filenames_df,\n",
    "                                                                  test_size=DATASET_PARAMS['SPLIT_RATIO'][-1],\n",
    "                                                                  random_state=DATASET_PARAMS['SEED'],\n",
    "                                                                  stratify=infeksi_filenames_df['source'])\n",
    "\n",
    "infeksi_train_filenames_df, infeksi_val_filenames_df = train_test_split(infeksi_trainval_filenames_df,\n",
    "                                                              test_size=DATASET_PARAMS['SPLIT_RATIO'][-2] * 1 / (1 - DATASET_PARAMS['SPLIT_RATIO'][-1]),\n",
    "                                                              random_state=DATASET_PARAMS['SEED'],\n",
    "                                                              stratify=infeksi_trainval_filenames_df['source'])\n",
    "\n",
    "# Chron\n",
    "chron_trainval_filenames_df, chron_test_filenames_df = train_test_split(chron_filenames_df,\n",
    "                                                                        test_size=DATASET_PARAMS['SPLIT_RATIO'][-1],\n",
    "                                                                        random_state=DATASET_PARAMS['SEED'],\n",
    "                                                                        stratify=chron_filenames_df['source'])\n",
    "\n",
    "chron_train_filenames_df, chron_val_filenames_df = train_test_split(chron_trainval_filenames_df,\n",
    "                                                                    test_size=DATASET_PARAMS['SPLIT_RATIO'][-2] * 1 / (1 - DATASET_PARAMS['SPLIT_RATIO'][-1]),\n",
    "                                                                    random_state=DATASET_PARAMS['SEED'],\n",
    "                                                                    stratify=chron_trainval_filenames_df['source'])\n",
    "\n",
    "# TB\n",
    "tb_trainval_filenames_df, tb_test_filenames_df = train_test_split(tb_filenames_df,\n",
    "                                                                    test_size=DATASET_PARAMS['SPLIT_RATIO'][-1],\n",
    "                                                                    random_state=DATASET_PARAMS['SEED'],\n",
    "                                                                    stratify=tb_filenames_df['source'])\n",
    "\n",
    "tb_train_filenames_df, tb_val_filenames_df = train_test_split(tb_trainval_filenames_df,\n",
    "                                                                    test_size=DATASET_PARAMS['SPLIT_RATIO'][-2] * 1\n",
    "                                                                    / (1 - DATASET_PARAMS['SPLIT_RATIO'][-1]),\n",
    "                                                                    random_state=DATASET_PARAMS['SEED'],\n",
    "                                                                    stratify=tb_trainval_filenames_df['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For TB: take all indo_cropped and indo_cropped_test sourced files and complete the rest with asan sourced file until 1000 files\n",
    "# tb_train_filenames_df_indo = tb_train_filenames_df[(tb_train_filenames_df['source'] == 'indo_cropped') | (tb_train_filenames_df['source'] == 'indo_cropped_test')]\n",
    "# tb_train_filenames_df_asan = tb_train_filenames_df[tb_train_filenames_df.index.isin(tb_train_filenames_df_indo.index) == False]\n",
    "\n",
    "# total_non_indo = 1000 - len(tb_train_filenames_df_indo)\n",
    "# tb_train_filenames_df_asan = tb_train_filenames_df_asan.sample(n=total_non_indo, random_state=DATASET_PARAMS['SEED'])\n",
    "\n",
    "# tb_train_filenames_df = pd.concat([tb_train_filenames_df_indo, tb_train_filenames_df_asan], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Infeksi, sample 1000 files\n",
    "# infeksi_train_filenames_df = infeksi_train_filenames_df.sample(n=1000, random_state=DATASET_PARAMS['SEED'], replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For UC, sample 500 from indo_cropped and indo_cropped_test and sample 250 from limuc_cropped_2 and changu_cropped\n",
    "# uc_train_filenames_df_indo = uc_train_filenames_df[(uc_train_filenames_df['source'] == 'indo_cropped') | (uc_train_filenames_df['source'] == 'indo_cropped_test')]\n",
    "# uc_train_filenames_df_limuc = uc_train_filenames_df[uc_train_filenames_df['source'] == 'limuc_cropped_2']\n",
    "# uc_train_filenames_df_changsu = uc_train_filenames_df[uc_train_filenames_df['source'] == 'changsu_cropped']\n",
    "\n",
    "# # Sample 500 from indo_cropped and indo_cropped_test\n",
    "# uc_train_filenames_df_indo = uc_train_filenames_df_indo.sample(n=len(uc_train_filenames_df_indo), random_state=DATASET_PARAMS['SEED'], replace=False)\n",
    "\n",
    "# # Sample each 250 from limuc_cropped_2 and changu_cropped\n",
    "# total_non_indo = 1000 - len(uc_train_filenames_df_indo)\n",
    "# total_each_non_indo = total_non_indo // 2\n",
    "\n",
    "# uc_train_filenames_df_limuc = uc_train_filenames_df_limuc.sample(n=total_each_non_indo, random_state=DATASET_PARAMS['SEED'], replace=False)\n",
    "# uc_train_filenames_df_changsu = uc_train_filenames_df_changsu.sample(n=total_each_non_indo, random_state=DATASET_PARAMS['SEED'], replace=False)\n",
    "\n",
    "# uc_train_filenames_df = pd.concat([uc_train_filenames_df_indo, uc_train_filenames_df_limuc, uc_train_filenames_df_changsu], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Chron, take all indo_cropped and indo_cropped_test sourced files and complete the rest with asan sourced file until 1000 files\n",
    "# chron_train_filenames_df_indo = chron_train_filenames_df[(chron_train_filenames_df['source'] == 'indo_cropped') | (chron_train_filenames_df['source'] == 'indo_cropped_test')]\n",
    "# chron_train_filenames_df_asan = chron_train_filenames_df[chron_train_filenames_df.index.isin(chron_train_filenames_df_indo.index) == False]\n",
    "\n",
    "# # Sample 500 from indo_cropped and indo_cropped_test\n",
    "# chron_train_filenames_df_indo = chron_train_filenames_df_indo.sample(n=len(chron_train_filenames_df_indo), random_state=DATASET_PARAMS['SEED'], replace=False)\n",
    "\n",
    "# # Sample 500 from asan\n",
    "# total_non_indo = 1000 - len(chron_train_filenames_df_indo)\n",
    "# chron_train_filenames_df_asan = chron_train_filenames_df_asan.sample(n=total_non_indo, random_state=DATASET_PARAMS['SEED'], replace=False)\n",
    "\n",
    "# chron_train_filenames_df = pd.concat([chron_train_filenames_df_indo, chron_train_filenames_df_asan], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames_df = pd.concat([uc_train_filenames_df, infeksi_train_filenames_df, chron_train_filenames_df, tb_train_filenames_df], ignore_index=True)\n",
    "val_filenames_df = pd.concat([uc_val_filenames_df, infeksi_val_filenames_df, chron_val_filenames_df, tb_val_filenames_df], ignore_index=True)\n",
    "test_filenames_df = pd.concat([uc_test_filenames_df, infeksi_test_filenames_df, chron_test_filenames_df, tb_test_filenames_df], ignore_index=True)\n",
    "\n",
    "# train_filenames_df = pd.concat([uc_train_filenames_df, infeksi_train_filenames_df], ignore_index=True)\n",
    "# val_filenames_df = pd.concat([uc_val_filenames_df, infeksi_val_filenames_df], ignore_index=True)\n",
    "# test_filenames_df = pd.concat([uc_test_filenames_df, infeksi_test_filenames_df], ignore_index=True)\n",
    "\n",
    "# train_filenames_df = pd.concat([chron_train_filenames_df, tb_train_filenames_df], ignore_index=True)\n",
    "# val_filenames_df = pd.concat([chron_val_filenames_df, tb_val_filenames_df], ignore_index=True)\n",
    "# test_filenames_df = pd.concat([chron_test_filenames_df, tb_test_filenames_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 'indo_cropped' and 'indo_cropped_test' sources for training and validation\n",
    "# train_filenames_df = train_filenames_df[(train_filenames_df['source'] == 'indo_cropped') | (train_filenames_df['source'] == 'indo_cropped_test') | (train_filenames_df['class'] == 'infeksi')]\n",
    "# val_filenames_df = val_filenames_df[(val_filenames_df['source'] == 'indo_cropped') | (val_filenames_df['source'] == 'indo_cropped_test') | (val_filenames_df['class'] == 'infeksi')]\n",
    "# test_filenames_df = test_filenames_df[(test_filenames_df['source'] == 'indo_cropped') | (test_filenames_df['source'] == 'indo_cropped_test') | (test_filenames_df['class'] == 'infeksi')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle train and validation sets\n",
    "train_filenames_df = train_filenames_df.sample(frac=1, random_state=DATASET_PARAMS['SEED']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Randomly choose infeksi class samples to balance the dataset\n",
    "# dataset_size = max([len(train_filenames_df[train_filenames_df['class'] == 'uc']), \n",
    "#                        len(train_filenames_df[train_filenames_df['class'] == 'chron']),\n",
    "#                        len(train_filenames_df[train_filenames_df['class'] == 'tb'])])\n",
    "\n",
    "# infeksi_samples_train_filesname_df = train_filenames_df[train_filenames_df['class'] == 'infeksi'].sample(n=dataset_size, random_state=DATASET_PARAMS['SEED'])\n",
    "# infeksi_samples_train_filesname_df_index = infeksi_samples_train_filesname_df.index\n",
    "\n",
    "# # Filter the train_filenames_df to include only the infeksi samples\n",
    "# train_filenames_df = train_filenames_df[(train_filenames_df['class'] != 'infeksi') | (train_filenames_df.index.isin(infeksi_samples_train_filesname_df_index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_PARAMS['CATEGORIES'] = ['uc', 'infeksi']\n",
    "# categories = ['uc', 'infeksi']\n",
    "\n",
    "# DATASET_PARAMS['CATEGORIES'] = ['chron', 'tb']\n",
    "# categories = ['chron', 'tb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "all_data_filenames_df = pd.concat([train_filenames_df, val_filenames_df, test_filenames_df], ignore_index=True)\n",
    "\n",
    "def change_indo(source):\n",
    "    if source == 'indo_cropped_test':\n",
    "        return 'indo_cropped'\n",
    "    return source\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "all_data_filenames_df['source'] = all_data_filenames_df['source'].apply(change_indo)\n",
    "\n",
    "# Group by then pivot\n",
    "all_data_filenames_df_grouped = all_data_filenames_df.groupby(['class', 'source']).size().reset_index(name='count')\n",
    "all_data_filenames_df_pivot = all_data_filenames_df_grouped.pivot(index='class', columns='source', values='count').fillna(0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_data_filenames_df_pivot.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "\n",
    "plt.title('Distribution of Classes in All Datasets')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.legend(title='Source', bbox_to_anchor=(1.01, 1.0175), loc='upper left')\n",
    "\n",
    "plt.ylim(0, 4000)  # Adjust the y-axis limit if needed\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "all_data_filenames_df = train_filenames_df.copy()\n",
    "\n",
    "def change_indo(source):\n",
    "    if source == 'indo_cropped_test':\n",
    "        return 'indo_cropped'\n",
    "    return source\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "all_data_filenames_df['source'] = all_data_filenames_df['source'].apply(change_indo)\n",
    "\n",
    "# Group by then pivot\n",
    "all_data_filenames_df_grouped = all_data_filenames_df.groupby(['class', 'source']).size().reset_index(name='count')\n",
    "all_data_filenames_df_pivot = all_data_filenames_df_grouped.pivot(index='class', columns='source', values='count').fillna(0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_data_filenames_df_pivot.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "\n",
    "plt.title('Distribution of Classes in Train Set')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.legend(title='Source', bbox_to_anchor=(1.01, 1.0175), loc='upper left')\n",
    "\n",
    "plt.ylim(0, 4000)  # Adjust the y-axis limit if needed\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "all_data_filenames_df = val_filenames_df.copy()\n",
    "\n",
    "def change_indo(source):\n",
    "    if source == 'indo_cropped_test':\n",
    "        return 'indo_cropped'\n",
    "    return source\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "all_data_filenames_df['source'] = all_data_filenames_df['source'].apply(change_indo)\n",
    "\n",
    "# Group by then pivot\n",
    "all_data_filenames_df_grouped = all_data_filenames_df.groupby(['class', 'source']).size().reset_index(name='count')\n",
    "all_data_filenames_df_pivot = all_data_filenames_df_grouped.pivot(index='class', columns='source', values='count').fillna(0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_data_filenames_df_pivot.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "\n",
    "plt.title('Distribution of Classes in Validation Set')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.legend(title='Source', bbox_to_anchor=(1.01, 1.0175), loc='upper left')\n",
    "\n",
    "plt.ylim(0, 400)  # Adjust the y-axis limit if needed\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "all_data_filenames_df = test_filenames_df.copy()\n",
    "\n",
    "def change_indo(source):\n",
    "    if source == 'indo_cropped_test':\n",
    "        return 'indo_cropped'\n",
    "    return source\n",
    "\n",
    "# Apply the function to the 'source' column\n",
    "all_data_filenames_df['source'] = all_data_filenames_df['source'].apply(change_indo)\n",
    "\n",
    "# Group by then pivot\n",
    "all_data_filenames_df_grouped = all_data_filenames_df.groupby(['class', 'source']).size().reset_index(name='count')\n",
    "all_data_filenames_df_pivot = all_data_filenames_df_grouped.pivot(index='class', columns='source', values='count').fillna(0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_data_filenames_df_pivot.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "\n",
    "plt.title('Distribution of Classes in Test Set')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.legend(title='Source', bbox_to_anchor=(1.01, 1.0175), loc='upper left')\n",
    "\n",
    "plt.ylim(0, 400)  # Adjust the y-axis limit if needed\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Delete \"src.xdl\" from sys.modules to avoid circular import issues\n",
    "if \"src.preprocessing\" in sys.modules:\n",
    "    del sys.modules[\"src.preprocessing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import preprocess\n",
    "\n",
    "target_input_size = tuple(PREPROCESSING_PARAMS['INPUT_SIZE'])\n",
    "train_transform = preprocess(\n",
    "    target_input_size=target_input_size,\n",
    "    rotation_range=PREPROCESSING_PARAMS['ROTATION_RANGE'],\n",
    "    width_shift_range=PREPROCESSING_PARAMS['WIDTH_SHIFT_RANGE'],\n",
    "    height_shift_range=PREPROCESSING_PARAMS['HEIGHT_SHIFT_RANGE'],\n",
    "    brightness_range=PREPROCESSING_PARAMS['BRIGHTNESS_RANGE'],\n",
    "    zoom_range=PREPROCESSING_PARAMS['ZOOM_RANGE'],\n",
    "    horizontal_flip=PREPROCESSING_PARAMS['HORIZONTAL_FLIP'],\n",
    "    vertical_flip=PREPROCESSING_PARAMS['VERTICAL_FLIP'],\n",
    "    channel_shift_range=PREPROCESSING_PARAMS['CHANNEL_SHIFT_RANGE'],\n",
    "    fill_mode=PREPROCESSING_PARAMS['FILL_MODE'],\n",
    "    shear_range=PREPROCESSING_PARAMS['SHEAR_RANGE']\n",
    "    )\n",
    "\n",
    "val_transform = preprocess(target_input_size=target_input_size) # only rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import Dataset\n",
    "\n",
    "train_dataset = Dataset(dataframe=train_filenames_df, \n",
    "                        categories=DATASET_PARAMS['CATEGORIES'],\n",
    "                        transform=train_transform, \n",
    "                        seed=42, \n",
    "                        shuffle=False)\n",
    "\n",
    "val_dataset = Dataset(dataframe=val_filenames_df, \n",
    "                      categories=DATASET_PARAMS['CATEGORIES'],\n",
    "                      transform=val_transform, \n",
    "                      seed=42, \n",
    "                      shuffle=False)\n",
    "\n",
    "test_dataset = Dataset(dataframe=test_filenames_df, \n",
    "                      categories=DATASET_PARAMS['CATEGORIES'],\n",
    "                      transform=val_transform, \n",
    "                      seed=42, \n",
    "                      shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "def new_source(instance):\n",
    "    if 'indo_cropped' in instance['source']:\n",
    "        return 'indo'\n",
    "    else:\n",
    "        return 'foreign'\n",
    "\n",
    "def combine_source_class(instance):\n",
    "    return f'{instance[\"source_str\"]}_{instance[\"class\"]}'\n",
    "\n",
    "train_filenames_df['source_str'] = train_filenames_df.apply(new_source, axis=1)\n",
    "train_filenames_df['group_key'] = train_filenames_df.apply(combine_source_class, axis=1)\n",
    "\n",
    "group_counts  = train_filenames_df[\"group_key\"].value_counts()            \n",
    "group_weights = 1.0 / group_counts                           \n",
    "\n",
    "group_weights['indo_uc'] = group_weights['indo_uc'] * 1.5\n",
    "group_weights['indo_tb'] = group_weights['indo_tb'] * 1.5\n",
    "group_weights['indo_chron'] = group_weights['indo_chron'] * 1.5\n",
    "\n",
    "group_weights['foreign_uc'] = group_weights['foreign_uc'] * 0.8\n",
    "group_weights['foreign_tb'] = group_weights['foreign_tb'] * 0.8\n",
    "group_weights['foreign_chron'] = group_weights['foreign_chron'] * 0.8\n",
    "\n",
    "sample_weights = train_filenames_df[\"group_key\"].map(group_weights).to_numpy()\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=TRAINING_PARAMS['BATCH_SIZE'], \n",
    "                          num_workers=TRAINING_PARAMS['NUM_WORKERS'],\n",
    "                          sampler=sampler)\n",
    "# )\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                          batch_size=TRAINING_PARAMS['BATCH_SIZE'], \n",
    "                          shuffle=False, \n",
    "                          num_workers=TRAINING_PARAMS['NUM_WORKERS'])\n",
    "\n",
    "#if test_dataset exist\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, \n",
    "                             batch_size=TRAINING_PARAMS['BATCH_SIZE'], \n",
    "                             shuffle=False, \n",
    "                             num_workers=TRAINING_PARAMS['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpect train data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def visualise_dataloader(dl, id_to_label=None, with_outputs=True):\n",
    "    total_num_images = len(dl.dataset)\n",
    "    idxs_seen = []\n",
    "    # class_0_batch_counts = []\n",
    "    # class_1_batch_counts = []\n",
    "    classes_batch_counts = {i: [] for i in range(len(id_to_label))} if id_to_label is not None else {0: [], 1: []}\n",
    "\n",
    "    for i, batch in enumerate(dl):\n",
    "\n",
    "        # idxs = batch[0][:, 0].tolist()\n",
    "        classes = np.argmax(batch[1], axis=1)\n",
    "\n",
    "        class_ids, class_counts = np.unique(classes, return_counts=True)\n",
    "        class_ids = set(class_ids.tolist())\n",
    "        class_counts = class_counts.tolist()\n",
    "\n",
    "        # idxs_seen.extend(idxs)\n",
    "\n",
    "        # if len(class_ids) == 2:\n",
    "        #     class_0_batch_counts.append(class_counts[0])\n",
    "        #     class_1_batch_counts.append(class_counts[1])\n",
    "        # elif len(class_ids) == 1 and 0 in class_ids:\n",
    "        #     class_0_batch_counts.append(class_counts[0])\n",
    "        #     class_1_batch_counts.append(0)\n",
    "        # elif len(class_ids) == 1 and 1 in class_ids:\n",
    "        #     class_0_batch_counts.append(0)\n",
    "        #     class_1_batch_counts.append(class_counts[0])\n",
    "        # else:\n",
    "            # raise ValueError(\"More than two classes detected\")\n",
    "\n",
    "        for class_id in range(len(id_to_label)):\n",
    "            if class_id in class_ids:\n",
    "                class_id_index = list(class_ids).index(class_id)\n",
    "                classes_batch_counts[class_id].append(class_counts[class_id_index])\n",
    "            else:\n",
    "                classes_batch_counts[class_id].append(0)\n",
    "\n",
    "    if with_outputs:\n",
    "        fig, ax = plt.subplots(1, figsize=(15, 15))\n",
    "\n",
    "        # ind = np.arange(len(class_0_batch_counts))\n",
    "        ind = np.arange(len(classes_batch_counts[0]))\n",
    "        width = 0.35\n",
    "\n",
    "        # ax.bar(\n",
    "        #     ind,\n",
    "        #     class_0_batch_counts,\n",
    "        #     width,\n",
    "        #     label=(id_to_label[0] if id_to_label is not None else \"0\"),\n",
    "        # )\n",
    "        # ax.bar(\n",
    "        #     ind + width,\n",
    "        #     class_1_batch_counts,\n",
    "        #     width,\n",
    "        #     label=(id_to_label[1] if id_to_label is not None else \"1\"),\n",
    "        # )\n",
    "        \n",
    "        for class_id, counts in classes_batch_counts.items():\n",
    "            ax.bar(\n",
    "                ind + (class_id * width),\n",
    "                counts,\n",
    "                width,\n",
    "                label=(id_to_label[class_id] if id_to_label is not None else str(class_id)),\n",
    "            )\n",
    "\n",
    "        ax.set_xticks(ind, ind + 1)\n",
    "        ax.set_xlabel(\"Batch index\", fontsize=12)\n",
    "        ax.set_ylabel(\"No. of images in batch\", fontsize=12)\n",
    "        ax.set_aspect(\"equal\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # num_images_seen = len(idxs_seen)\n",
    "\n",
    "        # print(\n",
    "        #     f'Avg Proportion of {(id_to_label[0] if id_to_label is not None else \"Class 0\")} per batch: {(np.array(class_0_batch_counts) / 10).mean()}'\n",
    "        # )\n",
    "        # print(\n",
    "        #     f'Avg Proportion of {(id_to_label[1] if id_to_label is not None else \"Class 1\")} per batch: {(np.array(class_1_batch_counts) / 10).mean()}'\n",
    "        # )\n",
    "        for class_id, counts in classes_batch_counts.items():\n",
    "            print(\n",
    "                f'Avg Proportion of {(id_to_label[class_id] if id_to_label is not None else str(class_id))} per batch: {(np.array(counts) / 10).mean()}'\n",
    "            )\n",
    "        print(\"=============\")\n",
    "        print(f\"Num. unique images seen: {len(set(idxs_seen))}/{total_num_images}\")\n",
    "\n",
    "    return classes_batch_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = visualise_dataloader(train_loader, id_to_label=DATASET_PARAMS['CATEGORIES'], with_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "data, target = next(iter(train_loader))\n",
    "\n",
    "img = data[random.randint(0, len(data) -1)].cpu().numpy()\n",
    "img = np.transpose(img, (1, 2, 0))\n",
    "img = img*255\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(f\"Augmented Train Images\\nTrue: {torch.argmax(target[random.randint(0, len(data) -1)], dim=0).item()}\")\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(categories)\n",
    "\n",
    "# Compute weights for each class based on the training dataset with sklearn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(class_weight='balanced', \n",
    "                                      classes=np.arange(num_classes), \n",
    "                                      y=train_filenames_df['class'].map(lambda x: categories.index(x)))\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientCapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"CAPSNET\":\n",
    "    from src.model import EfficientCapsNet\n",
    "    from src.loss import MarginLoss, marginLoss\n",
    "\n",
    "    model = EfficientCapsNet(input_size=(MODEL_PARAMS['INPUT_SIZE']))\n",
    "    loss = MarginLoss()\n",
    "    # loss = marginLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"DENSENET121\":\n",
    "    from src.densenet import DenseNet121\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "\n",
    "    model = DenseNet121(num_classes=num_classes, dropout_rate=MODEL_PARAMS['DROPOUT_RATE'])\n",
    "    loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"RESNET50\":\n",
    "    from src.resnet import ResNet50\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "\n",
    "    model = ResNet50(num_classes=num_classes, dropout_rate=MODEL_PARAMS['DROPOUT_RATE'])\n",
    "    loss = CrossEntropyLoss(weight=class_weights)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Get number of trainable parameters\n",
    "number_of_parameters = count_parameters(model)\n",
    "\n",
    "# Convert to string for printing\n",
    "number_of_parameters_str = f\"{number_of_parameters:,}\"\n",
    "\n",
    "print(f\"Number of trainable parameters: {number_of_parameters_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)*TRAINING_PARAMS['NUM_EPOCHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=TRAINING_PARAMS['LEARNING_RATE'])\n",
    "\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau, ExponentialLR\n",
    "\n",
    "# lr_scheduler = OneCycleLR(optimizer,\n",
    "#                           max_lr=TRAINING_PARAMS['LEARNING_RATE']*2,    \n",
    "#                           steps_per_epoch=len(train_loader),\n",
    "#                           epochs=TRAINING_PARAMS['NUM_EPOCHS'],\n",
    "#                           pct_start=0.2,\n",
    "#                           div_factor=5,\n",
    "#                           final_div_factor=20000,\n",
    "#                           )\n",
    "\n",
    "# lr_scheduler = ReduceLROnPlateau(optimizer,\n",
    "#                                  mode='min',\n",
    "#                                  factor=0.9,\n",
    "#                                  patience=3)\n",
    "\n",
    "# lr_scheduler = ExponentialLR(optimizer,\n",
    "                            #  gamma=0.95)\n",
    "\n",
    "lr_scheduler = None\n",
    "\n",
    "# use torcheval metrics\n",
    "# metrics\n",
    "from torcheval.metrics import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassF1Score,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassAUROC,\n",
    "    MulticlassAUPRC,\n",
    "    MulticlassRecall    \n",
    ")\n",
    "\n",
    "# Metrics\n",
    "from src.metrics import (\n",
    "    MulticlassMCC,\n",
    "    MulticlassSpecificity\n",
    ")\n",
    "\n",
    "metrics = {\n",
    "    \"mcc\": MulticlassMCC(num_classes=num_classes, device=DEVICE),\n",
    "    \"auprc\": MulticlassAUPRC(num_classes=num_classes, average= TRAINING_PARAMS['AVERAGE'], device=DEVICE),\n",
    "    \"auroc\": MulticlassAUROC(num_classes=num_classes, average= TRAINING_PARAMS['AVERAGE'], device=DEVICE),\n",
    "    \"accuracy\": MulticlassAccuracy(num_classes=num_classes, device=DEVICE),\n",
    "    \"f1_score\": MulticlassF1Score(num_classes=num_classes, average= TRAINING_PARAMS['AVERAGE'], device=DEVICE),\n",
    "    \"precision\": MulticlassPrecision(num_classes=num_classes, average=TRAINING_PARAMS['AVERAGE'], device = DEVICE),\n",
    "    \"recall\": MulticlassRecall(num_classes=num_classes, average=TRAINING_PARAMS['AVERAGE'], device = DEVICE),\n",
    "    \"specificity\": MulticlassSpecificity(num_classes=num_classes, average=TRAINING_PARAMS['AVERAGE'], device = DEVICE)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set only classifier to be trainable\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# for param in model.classifier.parameters():\n",
    "#     param.requires_grad = True\n",
    "    \n",
    "# # Unfreeze unless first layer in model.densenet_model.features\n",
    "# for param in model.densenet_model.features[-4:].parameters():\n",
    "#     param.requires_grad = True\n",
    "    \n",
    "    \n",
    "# Get number of trainable parameters\n",
    "number_of_parameters = count_parameters(model)\n",
    "\n",
    "# Convert to string for printing\n",
    "number_of_parameters_str = f\"{number_of_parameters:,}\"\n",
    "\n",
    "print(f\"Number of trainable parameters: {number_of_parameters_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import train\n",
    "\n",
    "history = train(model=model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    criterion=loss, \n",
    "    optimizer=optimizer, \n",
    "    num_epochs=TRAINING_PARAMS['NUM_EPOCHS'], \n",
    "    device=DEVICE,\n",
    "    metrics=metrics,\n",
    "    print_every=TRAINING_PARAMS['PRINT_EVERY'],\n",
    "    save_patience=TRAINING_PARAMS['SAVE_PATIENCE'],\n",
    "    save_path=TRAINING_PARAMS['SAVE_PATH'],\n",
    "    save_model=TRAINING_PARAMS['SAVE_MODEL'],\n",
    "    save_metrics=TRAINING_PARAMS['SAVE_METRICS'],\n",
    "    scheduler=lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"artifacts/uc_infeksi_chron_tb/densenet121/training_history.json\", \"r\") as f:\n",
    "    history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = history[\"train_loss\"]\n",
    "val_loss = history[\"val_loss\"]\n",
    "epoch = range(1, len(train_loss) + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epoch, train_loss, label='Training Loss')\n",
    "plt.plot(epoch, val_loss, label='Validation Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "using validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_model\n",
    "\n",
    "# model = load_model(\"artifacts/resnet50/epoch_15.pth\", num_classes=num_classes, dropout_rate=MODEL_PARAMS['DROPOUT_RATE'])\n",
    "# model = load_model(\"artifacts/densenet121/epoch_10.pth\", num_classes=2, dropout_rate=0.25)\n",
    "\n",
    "model.load_state_dict(torch.load(\"artifacts/uc_infeksi_chron_tb/densenet121/epoch_86_lr_0.000025_model_baru_best_dropout_0.3_weighted_sample_best_indo_full.pth\", map_location=DEVICE))\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(categories)\n",
    "# metrics\n",
    "from torcheval.metrics import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassF1Score,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassAUROC,\n",
    "    MulticlassAUPRC,\n",
    "    MulticlassRecall    \n",
    ")\n",
    "\n",
    "# Metrics\n",
    "from src.metrics import (\n",
    "    MulticlassMCC,\n",
    "    MulticlassSpecificity\n",
    ")\n",
    "\n",
    "test_metrics = {\n",
    "    \"mcc\": MulticlassMCC(num_classes=num_classes, device=DEVICE),\n",
    "    \"auprc\": MulticlassAUPRC(num_classes=num_classes, average= TRAINING_PARAMS['AVERAGE'], device=DEVICE),\n",
    "    \"auroc\": MulticlassAUROC(num_classes=num_classes, average= TRAINING_PARAMS['AVERAGE'], device=DEVICE),\n",
    "    \"accuracy\": MulticlassAccuracy(num_classes=num_classes, device=DEVICE),\n",
    "    \"f1_score\": MulticlassF1Score(num_classes=num_classes, average= TRAINING_PARAMS['AVERAGE'], device=DEVICE),\n",
    "    \"precision\": MulticlassPrecision(num_classes=num_classes, average=TRAINING_PARAMS['AVERAGE'], device = DEVICE),\n",
    "    \"recall\": MulticlassRecall(num_classes=num_classes, average=TRAINING_PARAMS['AVERAGE'], device = DEVICE),\n",
    "    \"specificity\": MulticlassSpecificity(num_classes=num_classes, average=TRAINING_PARAMS['AVERAGE'], device = DEVICE)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confussion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_variant_train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=TRAINING_PARAMS['BATCH_SIZE'], \n",
    "                          shuffle=False, \n",
    "                          num_workers=TRAINING_PARAMS['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.test import test\n",
    "\n",
    "raw_predictions, predicted_indices_flattened, target_indices_flattened, test_metrics = test(model = model, test_loader = test_loader, device = DEVICE, metrics=test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test_metrics to DataFrame\n",
    "test_metrics_df = pd.DataFrame(test_metrics, index=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(target_indices_flattened, predicted_indices_flattened)\n",
    "\n",
    "# Create figure and axis\n",
    "plt.figure(figsize=(7.5, 8/10*7.5))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=categories,\n",
    "            yticklabels=categories)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat raw_predictions (which is a list of tensor arrays) into a single tensor\n",
    "raw_predictions_cat = torch.cat(raw_predictions, dim=0)\n",
    "raw_predictions_np = raw_predictions_cat.cpu().numpy()\n",
    "\n",
    "from torch.nn.functional import softmax\n",
    "# Convert raw predictions to probabilities\n",
    "raw_predictions_prob = softmax(torch.tensor(raw_predictions_np), dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "from imblearn.metrics import specificity_score\n",
    "\n",
    "matthews_corrcoef_value = matthews_corrcoef(target_indices_flattened, predicted_indices_flattened)\n",
    "accuracy = accuracy_score(target_indices_flattened, predicted_indices_flattened)\n",
    "f1 = f1_score(target_indices_flattened, predicted_indices_flattened, average=None)\n",
    "precision = precision_score(target_indices_flattened, predicted_indices_flattened, average=None)\n",
    "recall = recall_score(target_indices_flattened, predicted_indices_flattened, average=None)\n",
    "specificity = specificity_score(target_indices_flattened, predicted_indices_flattened, average=None)\n",
    "roc_auc = roc_auc_score(target_indices_flattened, raw_predictions_prob, multi_class='ovr', average=None)\n",
    "average_precision = average_precision_score(target_indices_flattened, raw_predictions_prob, average=None)\n",
    "# roc_auc = roc_auc_score(target_indices_flattened, raw_predictions_prob[:, 1], multi_class='ovr', average=None)\n",
    "# average_precision = average_precision_score(target_indices_flattened, raw_predictions_prob[:, 1], average=None)\n",
    "\n",
    "# Convert metrics to DataFrame for better visualization\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1-Score', 'Precision', 'Recall', 'Specificity', 'ROC AUC (OVR)', 'ROC PRC', 'MCC'],\n",
    "    categories[0]: [accuracy, f1[0], precision[0], recall[0], specificity[0], roc_auc[0], average_precision[0], matthews_corrcoef_value],\n",
    "    categories[1]: [accuracy, f1[1], precision[1], recall[1], specificity[1], roc_auc[1], average_precision[1], matthews_corrcoef_value],\n",
    "    categories[2]: [accuracy, f1[2], precision[2], recall[2], specificity[2], roc_auc[2], average_precision[2], matthews_corrcoef_value],\n",
    "    categories[3]: [accuracy, f1[3], precision[3], recall[3], specificity[3], roc_auc[3], average_precision[3], matthews_corrcoef_value]\n",
    "})\n",
    "\n",
    "# metrics_df = pd.DataFrame({\n",
    "#     'Metric': ['Accuracy', 'F1-Score', 'Precision', 'Recall', 'Specificity', 'ROC AUC (OVR)', 'ROC PRC', 'MCC'],\n",
    "#     categories[0]: [accuracy, f1[0], precision[0], recall[0], specificity[0], roc_auc, average_precision, matthews_corrcoef_value],\n",
    "#     categories[1]: [accuracy, f1[1], precision[1], recall[1], specificity[1], roc_auc, average_precision, matthews_corrcoef_value]\n",
    "# })\n",
    "\n",
    "metrics_df.set_index('Metric', inplace=True)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test_filenames_df = test_filenames_df.copy()\n",
    "output_test_filenames_df['predicted'] = predicted_indices_flattened\n",
    "output_test_filenames_df['predicted'] = output_test_filenames_df['predicted'].apply(lambda x: categories[x])\n",
    "output_test_filenames_df\n",
    "\n",
    "indo_test_filenames_df = output_test_filenames_df[(output_test_filenames_df['source'] == 'indo_cropped') | (output_test_filenames_df['class'] == 'infeksi') | (output_test_filenames_df['source'] == 'indo_cropped_test')]\n",
    "\n",
    "target_indices_flattened_indo = target_indices_flattened[indo_test_filenames_df.index]\n",
    "predicted_indices_flattened_indo = predicted_indices_flattened[indo_test_filenames_df.index]\n",
    "raw_predictions_prob_indo = raw_predictions_prob[indo_test_filenames_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_corrcoef_value_indo = matthews_corrcoef(target_indices_flattened_indo, predicted_indices_flattened_indo)\n",
    "accuracy_indo = accuracy_score(target_indices_flattened_indo, predicted_indices_flattened_indo)\n",
    "f1_indo = f1_score(target_indices_flattened_indo, predicted_indices_flattened_indo, average=None)\n",
    "precision_indo = precision_score(target_indices_flattened_indo, predicted_indices_flattened_indo, average=None)\n",
    "recall_indo = recall_score(target_indices_flattened_indo, predicted_indices_flattened_indo, average=None)\n",
    "roc_auc_indo = roc_auc_score(target_indices_flattened_indo, raw_predictions_prob_indo, multi_class='ovr', average=None)\n",
    "average_precision_indo = average_precision_score(target_indices_flattened_indo, raw_predictions_prob_indo, average=None)\n",
    "specificity_indo = specificity_score(target_indices_flattened_indo, predicted_indices_flattened_indo, average=None)\n",
    "\n",
    "# Convert metrics to DataFrame for better visualization\n",
    "metrics_indo_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1-Score', 'Precision', 'Recall', 'Specificity', 'ROC AUC (OVR)', 'ROC PRC', 'MCC'],\n",
    "    'UC': [accuracy_indo, f1_indo[0], precision_indo[0], recall_indo[0], specificity_indo[0], roc_auc_indo[0], average_precision_indo[0], matthews_corrcoef_value_indo],\n",
    "    'Infeksi': [accuracy_indo, f1_indo[1], precision_indo[1], recall_indo[1], specificity_indo[1], roc_auc_indo[1], average_precision_indo[1], matthews_corrcoef_value_indo],\n",
    "    'CD': [accuracy_indo, f1_indo[2], precision_indo[2], recall_indo[2], specificity_indo[2], roc_auc_indo[2], average_precision_indo[2], matthews_corrcoef_value_indo],\n",
    "    'TB': [accuracy_indo, f1_indo[3], precision_indo[3], recall_indo[3], specificity_indo[3], roc_auc_indo[3], average_precision_indo[3], matthews_corrcoef_value_indo]\n",
    "})\n",
    "metrics_indo_df.set_index('Metric', inplace=True)\n",
    "metrics_indo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(target_indices_flattened_indo, predicted_indices_flattened_indo)\n",
    "\n",
    "# Create figure and axis\n",
    "plt.figure(figsize=(7.5, 8/10*7.5))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['UC', 'Infeksi', 'CD', 'TB'],\n",
    "            yticklabels=['UC', 'Infeksi', 'CD', 'TB'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image, sample_target = test_dataset[10]\n",
    "sample_image = sample_image.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(sample_image)\n",
    "    predicted_class = torch.argmax(prediction, dim=1).item()\n",
    "\n",
    "sample_image_np = sample_image.squeeze().cpu().numpy()\n",
    "sample_image_np = np.transpose(sample_image_np, (1, 2, 0))\n",
    "sample_image_np = sample_image_np*255\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(sample_image_np)\n",
    "plt.title(f'Predicted Class: {\"UC\" if predicted_class == 0 else \"Infeksi\"}\\n True Class: {\"UC\" if torch.argmax(sample_target).item() == 0 else \"Infeksi\"}\\nConfidence: {torch.max(prediction).item():.2f}')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"True class: {'UC' if torch.argmax(sample_target).item() == 0 else 'Infeksi'}\")\n",
    "print(f\"Predicted class: {'UC' if predicted_class == 0 else 'Infeksi'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Delete \"src.xdl\" from sys.modules to avoid circular import issues\n",
    "if \"src.xdl\" in sys.modules:\n",
    "    del sys.modules[\"src.xdl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Delete \"src.xdl\" from sys.modules to avoid circular import issues\n",
    "if \"src.xdl\" in sys.modules:\n",
    "    del sys.modules[\"src.xdl\"]\n",
    "\n",
    "from src.xdl import plot_XDL_GradCAM\n",
    "\n",
    "\n",
    "plot_XDL_GradCAM(model = model, \n",
    "                 test_loader = test_loader, \n",
    "                 device = DEVICE, \n",
    "                 print_img = True, \n",
    "                 num_samples = 100, \n",
    "                 save_path = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_train_len = len(train_filenames_df[train_filenames_df['class'] == 'uc'])\n",
    "uc_val_len = len(val_filenames_df[val_filenames_df['class'] == 'uc'])\n",
    "uc_test_len = len(test_filenames_df[test_filenames_df['class'] == 'uc'])\n",
    "\n",
    "print(f\"\"\"UC (LIMUC+Changsu) train: {uc_train_len}\n",
    "      \\nUC (LIMUC+Changsu) val: {uc_val_len}\n",
    "      \\nUC (LIMUC+Changsu) test: {uc_test_len}\n",
    "      \\nTotal: {uc_train_len + uc_val_len + uc_test_len}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.xdl import plot_XDL_SmoothGrad\n",
    "\n",
    "plot_XDL_SmoothGrad(model, \n",
    "                    test_loader, \n",
    "                    n_samples_smoothgrad=150,\n",
    "                    noise_level=0.1,\n",
    "                    device=DEVICE, \n",
    "                    num_samples=len(test_loader.dataset.dataframe), \n",
    "                    print_img=False, \n",
    "                    print_every=10, \n",
    "                    save_path=\"outputs/smoothgrad/limuc_cropped_novasir_2\",\n",
    "                    smoothgrad_percentile=97,\n",
    "                    smoothgrad_colormap='viridis',\n",
    "                    smoothgrad_overlay_alpha=0.4,\n",
    "                    fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.xdl import plot_XDL_SmoothGrad\n",
    "\n",
    "plot_XDL_SmoothGrad(model, \n",
    "                    test_loader, \n",
    "                    n_samples_smoothgrad=150,\n",
    "                    noise_level=0.1,\n",
    "                    device=DEVICE, \n",
    "                    num_samples=5, \n",
    "                    print_img=True, \n",
    "                    print_every=10, \n",
    "                    save_path=False,\n",
    "                    smoothgrad_percentile=91,\n",
    "                    smoothgrad_colormap='viridis',\n",
    "                    smoothgrad_overlay_alpha=0.6,\n",
    "                    fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.xdl import plot_XDL_SmoothGrad\n",
    "\n",
    "plot_XDL_SmoothGrad(model, \n",
    "                    test_loader, \n",
    "                    n_samples_smoothgrad=150,\n",
    "                    noise_level=0.1,\n",
    "                    device=DEVICE, \n",
    "                    num_samples=5, \n",
    "                    print_img=True, \n",
    "                    print_every=10, \n",
    "                    save_path=False,\n",
    "                    smoothgrad_percentile=98,\n",
    "                    smoothgrad_colormap='hot',\n",
    "                    smoothgrad_overlay_alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM + Smoothgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Delete \"src.xdl\" from sys.modules to avoid circular import issues\n",
    "if \"src.xdl\" in sys.modules:\n",
    "    del sys.modules[\"src.xdl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test_filenames_df = output_test_filenames_df[output_test_filenames_df['source'].isin(['indo_cropped', 'indo_cropped_test'])].copy()\n",
    "output_test_filenames_df['predicted_class'] = output_test_filenames_df['predicted']\n",
    "# Sample 100 images from the test set for each class\n",
    "# output_test_filenames_df = output_test_filenames_df.groupby('class').apply(lambda x: x.sample(n=100, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# Randomly shuffle the sampled images\n",
    "output_test_filenames_df = output_test_filenames_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Make Dataset\n",
    "output_test_dataset = Dataset(dataframe=output_test_filenames_df, \n",
    "                              categories=DATASET_PARAMS['CATEGORIES'],\n",
    "                              transform=val_transform, \n",
    "                              seed=42, \n",
    "                              shuffle=True)\n",
    "\n",
    "# Make DataLoader\n",
    "output_test_loader = DataLoader(output_test_dataset, \n",
    "                                 batch_size=TRAINING_PARAMS['BATCH_SIZE'], \n",
    "                                 num_workers=TRAINING_PARAMS['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.xdl import plot_XDL_Visualizations\n",
    "\n",
    "plot_XDL_Visualizations(model, \n",
    "                        output_test_loader, \n",
    "                        device=DEVICE, \n",
    "                        num_samples=len(output_test_loader.dataset.dataframe), \n",
    "                        print_img=True, \n",
    "                        print_every=5, \n",
    "                        save_path=\"outputs/xdl/four_class_17_08_2025_all\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tugas-akhir-cloned",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
